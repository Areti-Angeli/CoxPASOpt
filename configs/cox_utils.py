# -*- coding: utf-8 -*-
"""cox_utils.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EqEq0xCxxZefjhPNu1uiVvC2Q57HDU4B
"""

## Model

class Cox_PASNet(nn.Module):
	def __init__(self, In_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, pathway_mask):
			super(Cox_PASNet, self).__init__()

			self.pathway_mask = pathway_mask

			self.sc1 = nn.Linear(In_Nodes, Pathway_Nodes)
			self.sc2 = nn.Linear(Pathway_Nodes, Hidden_Nodes)
			self.sc3 = nn.Linear(Hidden_Nodes, Hidden_Nodes)
			self.sc4 = nn.Linear(Hidden_Nodes + 1, Out_Nodes)


			self.do_m1 = None
			self.do_m2 = None

			self.bn1 = nn.BatchNorm1d(Pathway_Nodes)
			self.bn2 = nn.BatchNorm1d(Hidden_Nodes)
			self.bn3 = nn.BatchNorm1d(Hidden_Nodes)

			self.tanh = nn.Tanh()

			###if gpu is being used
			if torch.cuda.is_available():
				self.do_m1 = self.do_m1.cuda()
				self.do_m2 = self.do_m2.cuda()

	def forward(self, x_1, x_2):
		x_1 = torch.relu(self.bn1(self.sc1(x_1)))
		if self.training:
				x_1 = x_1.mul(self.do_m1)
		x_1 = torch.relu(self.bn2(self.sc2(x_1)))
		if self.training:
				x_1 = x_1.mul(self.do_m2)
		x_1 = torch.relu(self.bn3(self.sc3(x_1)))

		x_2 = x_2.view(-1, 1)
		###combine age with hidden layer 2
		x_cat = torch.cat((x_1, x_2), 1)
		lin_pred = self.sc4(x_cat)

		return lin_pred

## Subnetwork sparse code

def dropout_mask(n_node, drop_p):
	'''Construct a binary matrix to randomly drop nodes in a layer.
	Input:
		n_node: number of nodes in the layer.
		drop_p: the probability that a node is to be dropped.
	Output:
		mask: a binary matrix, where 1 --> keep the node; 0 --> drop the node.
	'''
	keep_p = 1.0 - drop_p
	mask = torch.Tensor(np.random.binomial(1, keep_p, size=n_node))
	###if gpu is being used
	if torch.cuda.is_available():
		mask = mask.cuda()
	###
	return mask

def s_mask(sparse_level, param_matrix, nonzero_param_1D, dtype):
	'''Construct a binary matrix w.r.t. a sparsity level of weights between two consecutive layers
	Input:
		sparse_level: a percentage value in [0, 100) represents the proportion of weights in a sub-network to be dropped.
		param_matrix: a weight matrix for entrie network.
		nonzero_param_1D: 1D of non-zero 'param_matrix' (which is the weights selected from a sub-network).
		dtype: define the data type of tensor (i.e. dtype=torch.FloatTensor).
	Output:
		param_mask: a binary matrix, where 1 --> keep the node; 0 --> drop the node.
	'''
	###take the absolute values of param_1D
	non_neg_param_1D = torch.abs(nonzero_param_1D)
	###obtain the number of params
	num_param = nonzero_param_1D.size(0)
	###obtain the kth number based on sparse_level
	top_k = math.ceil(num_param*(100-sparse_level)*0.01)
	###obtain the k largest params
	sorted_non_neg_param_1D, indices = torch.topk(non_neg_param_1D, top_k)
	param_mask = torch.abs(param_matrix) > sorted_non_neg_param_1D.min()
	param_mask = param_mask.type(dtype)
	###if gpu is being used
	if torch.cuda.is_available():
		param_mask = param_mask.cuda()
	###
	return param_mask

## Train Model

dtype = torch.FloatTensor

def trainCoxPASNet(train_x, train_age, train_ytime, train_yevent, \
			eval_x, eval_age, eval_ytime, eval_yevent, pathway_mask, \
			In_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, \
			Learning_Rate, L2, Num_Epochs, Dropout_Rate):

	net = Cox_PASNet(In_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, pathway_mask)
	###if gpu is being used
	if torch.cuda.is_available():
		net.cuda()
	###
	###optimizer
	losses_train = []
	losses_eval = []
	train_cindexes = []
	eval_cindexes = []

	opt = optim.Adam(net.parameters(), lr=Learning_Rate, weight_decay = L2)

	for epoch in range(Num_Epochs+1):
		net.train()
		opt.zero_grad()
		###Randomize dropout masks
		net.do_m1 = dropout_mask(Pathway_Nodes, Dropout_Rate[0])
		net.do_m2 = dropout_mask(Hidden_Nodes, Dropout_Rate[1])

		# Forward pass for training data
		pred_train = net(train_x, train_age)
		loss_train = neg_par_log_likelihood(pred_train, train_ytime, train_yevent)
	  # Modification to handle potential multi-element loss
		if isinstance(loss_train, torch.Tensor) and len(loss_train.shape) > 0:
				loss_train = loss_train.mean()

		losses_train.append(loss_train.item())  # Log the training loss value
		# Backward pass and optimization
		loss_train.backward() ###calculate gradients
		opt.step() ###update weights and biases

		net.sc1.weight.data = net.sc1.weight.data.mul(net.pathway_mask) ###force the connections between gene layer and pathway layer

		# Calculate validation loss
		net.eval()
		with torch.no_grad():
			pred_eval = net(eval_x, eval_age)
			loss_eval = neg_par_log_likelihood(pred_eval, eval_ytime, eval_yevent)
			# Modification to handle potential multi-element loss
			if isinstance(loss_eval, torch.Tensor) and len(loss_eval.shape) > 0:
					loss_eval = loss_eval.mean()
			losses_eval.append(loss_eval.item())


		# Calculate C-index for training and validation data
		train_cindex = c_index(pred_train, train_ytime, train_yevent)
		eval_cindex = c_index(pred_eval, eval_ytime, eval_yevent)
		train_cindexes.append(train_cindex)
		eval_cindexes.append(eval_cindex)

		###obtain the small sub-network's connections
		do_m1_grad = copy.deepcopy(net.sc2.weight._grad.data)
		do_m2_grad = copy.deepcopy(net.sc3.weight._grad.data)
		do_m1_grad_mask = torch.where(do_m1_grad == 0, do_m1_grad, torch.ones_like(do_m1_grad))
		do_m2_grad_mask = torch.where(do_m2_grad == 0, do_m2_grad, torch.ones_like(do_m2_grad))
		###copy the weights
		net_sc2_weight = copy.deepcopy(net.sc2.weight.data)
		net_sc3_weight = copy.deepcopy(net.sc3.weight.data)

		###serializing net
		net_state_dict = net.state_dict()

		###Sparse Coding
		###make a copy for net, and then optimize sparsity level via copied net
		copy_net = copy.deepcopy(net)
		copy_state_dict = copy_net.state_dict()
		for name, param in copy_state_dict.items():
			###omit the param if it is not a weight matrix
			if not "weight" in name:
				continue
			###omit gene layer
			if "sc1" in name:
				continue
			###stop sparse coding
			if "sc4" in name:
				break
			###sparse coding between the current two consecutive layers is in the trained small sub-network
			if "sc2" in name:
				active_param = net_sc2_weight.mul(do_m1_grad_mask)
			if "sc3" in name:
				active_param = net_sc3_weight.mul(do_m2_grad_mask)
			nonzero_param_1d = active_param[active_param != 0]
			if nonzero_param_1d.size(0) == 0:
				break
			copy_param_1d = copy.deepcopy(nonzero_param_1d)
			###set up potential sparsity level in [0, 100)
			S_set =  torch.arange(100, -1, -1)[1:]
			copy_param = copy.deepcopy(active_param)
			S_loss = []
			for S in S_set:
				param_mask = s_mask(sparse_level = S.item(), param_matrix = copy_param, nonzero_param_1D = copy_param_1d, dtype = dtype)
				transformed_param = copy_param.mul(param_mask)
				copy_state_dict[name].copy_(transformed_param)
				copy_net.train()
				y_tmp = copy_net(train_x, train_age)
				loss_tmp = neg_par_log_likelihood(y_tmp, train_ytime, train_yevent)

				S_loss.append(loss_tmp.mean().item() if isinstance(loss_tmp, torch.Tensor) else loss_tmp)

			###apply cubic interpolation
			# Convert S_loss elements to float if they are PyTorch scalars
			S_loss_values = [loss.item() if isinstance(loss, torch.Tensor) else loss for loss in S_loss]
			interp_S_loss = interp1d(S_set.detach().numpy(), S_loss_values, kind='cubic')
			interp_S_set = torch.linspace(min(S_set), max(S_set), steps=100)
			interp_loss = interp_S_loss(interp_S_set)
			optimal_S = interp_S_set[np.argmin(interp_loss)]
			optimal_param_mask = s_mask(sparse_level = optimal_S.item(), param_matrix = copy_param, nonzero_param_1D = copy_param_1d, dtype = dtype)
			if "sc2" in name:
				final_optimal_param_mask = torch.where(do_m1_grad_mask == 0, torch.ones_like(do_m1_grad_mask), optimal_param_mask)
				optimal_transformed_param = net_sc2_weight.mul(final_optimal_param_mask)
			if "sc3" in name:
				final_optimal_param_mask = torch.where(do_m2_grad_mask == 0, torch.ones_like(do_m2_grad_mask), optimal_param_mask)
				optimal_transformed_param = net_sc3_weight.mul(final_optimal_param_mask)
			###update weights in copied net
			copy_state_dict[name].copy_(optimal_transformed_param)
			###update weights in net
			net_state_dict[name].copy_(optimal_transformed_param)

		if epoch % 200 == 0:
			net.train()
			train_pred = net(train_x, train_age)
			# Calculate the mean of the loss if it's a multi-element tensor
			train_loss = neg_par_log_likelihood(train_pred, train_ytime, train_yevent)
			train_loss = train_loss.mean() if isinstance(train_loss, torch.Tensor) and len(train_loss.shape) > 0 else train_loss
			net.eval()
			eval_pred = net(eval_x, eval_age)
			# Calculate the mean of the loss if it's a multi-element tensor
			eval_loss = neg_par_log_likelihood(eval_pred, eval_ytime, eval_yevent)
			eval_loss = eval_loss.mean() if isinstance(eval_loss, torch.Tensor) and len(eval_loss.shape) > 0 else eval_loss

			train_cindex = c_index(train_pred, train_ytime, train_yevent)
			eval_cindex = c_index(eval_pred, eval_ytime, eval_yevent)
			print(f"Epoch {epoch} - Loss in Train: {train_loss.item()} - Loss in Validation: {eval_loss.item()}")
			print(f"Training C-index: {train_cindex}, Validation C-index: {eval_cindex}")

	# Plot the training and validation loss after training
	plt.figure(figsize=(10, 6))
	plt.plot(losses_train, label='Training Loss')
	plt.plot(losses_eval, label='Validation Loss')
	plt.xlabel('Epoch')
	plt.ylabel('Loss')
	plt.title('Training and Validation Loss Over Epochs')
	plt.legend()
	plt.grid(True)
	plt.show()

  # Plot the C-index values after training
	plt.figure(figsize=(10, 6))
	plt.plot(train_cindexes, label='Training C-index')
	plt.plot(eval_cindexes, label='Validation C-index')
	plt.xlabel('Epoch')
	plt.ylabel('C-index')
	plt.title('Training and Validation C-index Over Epochs')
	plt.legend()
	plt.grid(True)
	plt.show()


	return (train_loss, eval_loss, train_cindex, eval_cindex)

dtype = torch.FloatTensor

def InterpretCoxPASNet(x, age, ytime, yevent, pathway_mask, \
						In_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, \
						Learning_Rate, L2, Num_Epochs, Dropout_Rate, outpath):

	net = Cox_PASNet(In_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, pathway_mask)
	###if gpu is being used
	if torch.cuda.is_available():
		net.cuda()
	###
	###optimizer
	opt = optim.Adam(net.parameters(), lr=Learning_Rate, weight_decay = L2)

	for epoch in range(Num_Epochs+1):
		net.train()
		opt.zero_grad() ###reset gradients to zeros
		###Randomize dropout masks
		net.do_m1 = dropout_mask(Pathway_Nodes, Dropout_Rate[0])
		net.do_m2 = dropout_mask(Hidden_Nodes, Dropout_Rate[1])

		pred = net(x, age) ###Forward
		loss = neg_par_log_likelihood(pred, ytime, yevent).mean()

		loss.backward() ###calculate gradients
		opt.step() ###update weights and biases

		net.sc1.weight.data = net.sc1.weight.data.mul(net.pathway_mask)

		###obtain the small sub-network's connections
		do_m1_grad = copy.deepcopy(net.sc2.weight._grad.data)
		do_m2_grad = copy.deepcopy(net.sc3.weight._grad.data)
		do_m1_grad_mask = torch.where(do_m1_grad == 0, do_m1_grad, torch.ones_like(do_m1_grad))
		do_m2_grad_mask = torch.where(do_m2_grad == 0, do_m2_grad, torch.ones_like(do_m2_grad))
		###copy the weights
		net_sc2_weight = copy.deepcopy(net.sc2.weight.data)
		net_sc3_weight = copy.deepcopy(net.sc3.weight.data)

		###serializing net
		net_state_dict = net.state_dict()

		###Sparse Coding
		###make a copy for net, and then optimize sparsity level via copied net
		copy_net = copy.deepcopy(net)
		copy_state_dict = copy_net.state_dict()
		for name, param in copy_state_dict.items():
			###omit the param if it is not a weight matrix
			if not "weight" in name:
				continue
			###omit gene layer
			if "sc1" in name:
				continue
			###stop sparse coding
			if "sc4" in name:
				break
			###sparse coding between the current two consecutive layers is in the trained small sub-network
			if "sc2" in name:
				active_param = net_sc2_weight.mul(do_m1_grad_mask)
			if "sc3" in name:
				active_param = net_sc3_weight.mul(do_m2_grad_mask)
			nonzero_param_1d = active_param[active_param != 0]
			if nonzero_param_1d.size(0) == 0: ###stop sparse coding between the current two consecutive layers if there are no valid weights
				break
			copy_param_1d = copy.deepcopy(nonzero_param_1d)
			###set up potential sparsity level in [0, 100)
			S_set =  torch.arange(100, -1, -1)[1:]
			copy_param = copy.deepcopy(active_param)
			S_loss = []
			for S in S_set:
					param_mask = s_mask(sparse_level = S.item(), param_matrix = copy_param, nonzero_param_1D = copy_param_1d, dtype = dtype)
					transformed_param = copy_param.mul(param_mask)
					copy_state_dict[name].copy_(transformed_param)
					copy_net.train()
					y_tmp = copy_net(x, age)
					loss_tmp = neg_par_log_likelihood(y_tmp, ytime, yevent)
					# Ensure loss_tmp is a scalar by taking the mean
					loss_tmp = loss_tmp.mean()
					S_loss.append(loss_tmp.item())
			###apply cubic interpolation
			interp_S_loss = interp1d(S_set.detach().numpy(), S_loss, kind='cubic')
			interp_S_set = torch.linspace(min(S_set), max(S_set), steps=100)
			interp_loss = interp_S_loss(interp_S_set)
			optimal_S = interp_S_set[np.argmin(interp_loss)]
			optimal_param_mask = s_mask(sparse_level = optimal_S.item(), param_matrix = copy_param, nonzero_param_1D = copy_param_1d, dtype = dtype)
			if "sc2" in name:
				final_optimal_param_mask = torch.where(do_m1_grad_mask == 0, torch.ones_like(do_m1_grad_mask), optimal_param_mask)
				optimal_transformed_param = net_sc2_weight.mul(final_optimal_param_mask)
			if "sc3" in name:
				final_optimal_param_mask = torch.where(do_m2_grad_mask == 0, torch.ones_like(do_m2_grad_mask), optimal_param_mask)
				optimal_transformed_param = net_sc3_weight.mul(final_optimal_param_mask)
			###update weights in copied net
			copy_state_dict[name].copy_(optimal_transformed_param)
			###update weights in net
			net_state_dict[name].copy_(optimal_transformed_param)

	###save the trained model
	torch.save(net.state_dict(), outpath)


	net.eval()  # Switch to evaluation mode
	with torch.no_grad():
			final_pred = net(x, age)  # Forward pass with the full dataset
			final_loss = neg_par_log_likelihood(final_pred, ytime, yevent).mean()
			print(f"Final Loss: {final_loss.item()}")

	return