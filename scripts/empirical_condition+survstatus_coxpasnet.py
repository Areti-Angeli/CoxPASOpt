# -*- coding: utf-8 -*-
"""empirical_condition+survstatus_coxpasnet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k21RphOQTWr5J5JPRNVM9VTpRom3EL2k
"""

import os
from google.colab import drive
drive.mount("/content/drive/", force_remount=True)
root_dir = "/content/drive/MyDrive/"
project_folder = "jan_files"
os.chdir(root_dir + project_folder)
!pwd

import os
import pandas as pd
import pickle

df = pd.read_excel("pt.xlsx")

df['target_label'] = df['target_label'].str.split(';')

df_n = df.explode("target_label").pivot_table(index="source_label", columns="target_label", aggfunc="size", fill_value=0).reset_index()
df_n = df_n.set_index('source_label')

df_n.to_excel("pt_fixed.xlsx")

#Install NumPy 1.23.5
!pip install numpy==1.23.5

!pip install shap==0.42.0
import shap

import shap
import numpy as np
import pandas as pd
import math
import copy
from scipy.interpolate import interp1d
import matplotlib.pyplot as plt

import torch
import torch.nn.functional as F
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

from sklearn.metrics import roc_auc_score, f1_score

torch.manual_seed(0)

"""### Data Loader

"""

def sort_data(path):
	''' sort the genomic and clinical data w.r.t. survival time (OS_MONTHS) in descending order
	Input:
		path: path to input dataset (which is expected to be a csv file).
	Output:
		x: sorted genomic inputs.
		ytime: sorted survival time (OS_MONTHS) corresponding to 'x'.
		yevent: sorted censoring status (OS_EVENT) corresponding to 'x', where 1 --> deceased; 0 --> censored.
		age: sorted age corresponding to 'x'.
	'''

	data = pd.read_excel(path)

	data.sort_values("OS_MONTHS", ascending = False, inplace = True)

	x = data.drop(columns=["SAMPLE_ID", "OS_MONTHS", "OS_EVENT", "AGE", "Condition"]).values.astype(np.float32)
	ytime = data["OS_MONTHS"].values.astype(np.float32).reshape(-1, 1)
	yevent = data["OS_EVENT"].values.astype(np.float32).reshape(-1, 1)
	age = data["AGE"].values.astype(np.float32).reshape(-1, 1)
	condition = data["Condition"].values.astype(np.float32).reshape(-1, 1)

	feature_names = list(data.drop(columns=["SAMPLE_ID", "OS_MONTHS", "OS_EVENT", "AGE", "Condition"]).columns)

	return(x, ytime, yevent, age, condition, feature_names)


def load_data(path, dtype):
	'''Load the sorted data, and then covert it to a Pytorch tensor.
	Input:
		path: path to input dataset (which is expected to be a csv file).
		dtype: define the data type of tensor (i.e. dtype=torch.FloatTensor)
	Output:
		X: a Pytorch tensor of 'x' from sort_data().
		YTIME: a Pytorch tensor of 'ytime' from sort_data().
		YEVENT: a Pytorch tensor of 'yevent' from sort_data().
		AGE: a Pytorch tensor of 'age' from sort_data().
	'''
	x, ytime, yevent, age, condition, feature_names = sort_data(path)
	X = torch.from_numpy(x).type(dtype)
	YTIME = torch.from_numpy(ytime).type(dtype)
	YEVENT = torch.from_numpy(yevent).type(dtype)
	AGE = torch.from_numpy(age).type(dtype)
	CONDITION = torch.from_numpy(condition).type(dtype)
	###if gpu is being used
	if torch.cuda.is_available():
		X = X.cuda()
		YTIME = YTIME.cuda()
		YEVENT = YEVENT.cuda()
		AGE = AGE.cuda()
		CONDITION = CONDITION.cuda()
	###
	return(X, YTIME, YEVENT, AGE, CONDITION, feature_names)


def load_pathway(path, dtype):
	'''Load a bi-adjacency matrix of pathways, and then covert it to a Pytorch tensor.
	Input:
		path: path to input dataset (which is expected to be a csv file).
		dtype: define the data type of tensor (i.e. dtype=torch.FloatTensor)
	Output:
		PATHWAY_MASK: a Pytorch tensor of the bi-adjacency matrix of pathways.
	'''
	pathway_mask = pd.read_excel(path, index_col = 0).to_numpy()

	PATHWAY_MASK = torch.from_numpy(pathway_mask).type(dtype)
	###if gpu is being used
	if torch.cuda.is_available():
		PATHWAY_MASK = PATHWAY_MASK.cuda()
	###
	return(PATHWAY_MASK)

"""### Survival Cost Function"""

def R_set(x):
	'''Create an indicator matrix of risk sets, where T_j >= T_i.
	Note that the input data have been sorted in descending order.
	Input:
		x: a PyTorch tensor that the number of rows is equal to the number of samples.
	Output:
		indicator_matrix: an indicator matrix (which is a lower traiangular portions of matrix).
	'''
	n_sample = x.size(0)
	matrix_ones = torch.ones(n_sample, n_sample)
	indicator_matrix = torch.tril(matrix_ones)

	return(indicator_matrix)


def neg_par_log_likelihood(pred, ytime, yevent):
	'''Calculate the average Cox negative partial log-likelihood.
	Note that this function requires the input data have been sorted in descending order.
	Input:
		pred: linear predictors from trained model.
		ytime: true survival time from load_data().
		yevent: true censoring status from load_data().
	Output:
		cost: the cost that is to be minimized.
	'''
	n_observed = yevent.sum(0)
	ytime_indicator = R_set(ytime)
	###if gpu is being used
	if torch.cuda.is_available():
		ytime_indicator = ytime_indicator.cuda()
	###
	risk_set_sum = ytime_indicator.mm(torch.exp(pred))
	diff = pred - torch.log(risk_set_sum)
	sum_diff_in_observed = torch.transpose(diff, 0, 1).mm(yevent)
	cost = (- (sum_diff_in_observed / n_observed)).reshape((-1,))

	return(cost)


def c_index(pred, ytime, yevent):
    '''Calculate concordance index to evaluate the performance of survival prediction.
    Input:
        pred: linear predictor for the Cox model.
        ytime: survival time
        yevent: censoring status (1: event; 0: censor)
    Output:
        c_index: the concordance index calculated based on Harrell's definition.
    '''
    n_sample = len(ytime)
    pred_matrix = np.zeros((n_sample, n_sample))
    time_matrix = np.zeros((n_sample, n_sample))
    event_matrix = np.zeros((n_sample, n_sample))

    for i in range(n_sample):
        for j in range(n_sample):
            time_matrix[i, j] = ytime[j] - ytime[i]
            event_matrix[i, j] = yevent[j]

    # Ensure pred is a 1D tensor and access elements directly
    pred = pred.view(-1)

    for j in range(n_sample):
        for i in range(n_sample):
            # Use .item() to get a single value from the tensor
            if pred[i].item() < pred[j].item():
                pred_matrix[j, i] = 1
            elif pred[i].item() == pred[j].item():
                pred_matrix[j, i] = 0.5

    concordant = 0
    discordant = 0
    tied_risk = 0
    comparable = 0

    for i in range(n_sample):
        for j in range(i + 1, n_sample):
            if time_matrix[i, j] > 0 and event_matrix[i, j] == 1:
                comparable += 1
                if pred_matrix[i, j] == 1:
                    concordant += 1
                elif pred_matrix[i, j] == 0:
                    discordant += 1
                else:
                    tied_risk += 1
            elif time_matrix[i, j] < 0 and event_matrix[j, i] == 1:
                comparable += 1
                if pred_matrix[j, i] == 1:
                    concordant += 1
                elif pred_matrix[j, i] == 0:
                    discordant += 1
                else:
                    tied_risk += 1

    c_index = (concordant + 0.5 * tied_risk) / comparable
    return c_index

"""### Model

"""

class Cox_PASNet(nn.Module):
	def __init__(self, In_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, pathway_mask):
			super(Cox_PASNet, self).__init__()

			self.pathway_mask = pathway_mask

			self.sc1 = nn.Linear(In_Nodes, Pathway_Nodes)
			self.sc2 = nn.Linear(Pathway_Nodes, Hidden_Nodes)
			self.sc3 = nn.Linear(Hidden_Nodes, Hidden_Nodes)
			self.sc4 = nn.Linear(Hidden_Nodes + 1, Out_Nodes)


			self.do_m1 = None
			self.do_m2 = None

			self.bn1 = nn.BatchNorm1d(Pathway_Nodes)
			self.bn2 = nn.BatchNorm1d(Hidden_Nodes)
			self.bn3 = nn.BatchNorm1d(Hidden_Nodes)

			self.tanh = nn.Tanh()

			###if gpu is being used
			if torch.cuda.is_available():
				self.do_m1 = self.do_m1.cuda()
				self.do_m2 = self.do_m2.cuda()

	def forward(self, x_1, x_2):
		x_1 = torch.relu(self.bn1(self.sc1(x_1)))
		if self.training:
				x_1 = x_1.mul(self.do_m1)
		x_1 = torch.relu(self.bn2(self.sc2(x_1)))
		if self.training:
				x_1 = x_1.mul(self.do_m2)
		x_1 = torch.relu(self.bn3(self.sc3(x_1)))

		x_2 = x_2.view(-1, 1)
		###combine age with hidden layer 2
		x_cat = torch.cat((x_1, x_2), 1)
		lin_pred = self.sc4(x_cat)

		return lin_pred

"""### Subnetwork sparse code"""

def dropout_mask(n_node, drop_p):
	'''Construct a binary matrix to randomly drop nodes in a layer.
	Input:
		n_node: number of nodes in the layer.
		drop_p: the probability that a node is to be dropped.
	Output:
		mask: a binary matrix, where 1 --> keep the node; 0 --> drop the node.
	'''
	keep_p = 1.0 - drop_p
	mask = torch.Tensor(np.random.binomial(1, keep_p, size=n_node))
	###if gpu is being used
	if torch.cuda.is_available():
		mask = mask.cuda()
	###
	return mask

def s_mask(sparse_level, param_matrix, nonzero_param_1D, dtype):
	'''Construct a binary matrix w.r.t. a sparsity level of weights between two consecutive layers
	Input:
		sparse_level: a percentage value in [0, 100) represents the proportion of weights in a sub-network to be dropped.
		param_matrix: a weight matrix for entrie network.
		nonzero_param_1D: 1D of non-zero 'param_matrix' (which is the weights selected from a sub-network).
		dtype: define the data type of tensor (i.e. dtype=torch.FloatTensor).
	Output:
		param_mask: a binary matrix, where 1 --> keep the node; 0 --> drop the node.
	'''
	###take the absolute values of param_1D
	non_neg_param_1D = torch.abs(nonzero_param_1D)
	###obtain the number of params
	num_param = nonzero_param_1D.size(0)
	###obtain the kth number based on sparse_level
	top_k = math.ceil(num_param*(100-sparse_level)*0.01)
	###obtain the k largest params
	sorted_non_neg_param_1D, indices = torch.topk(non_neg_param_1D, top_k)
	param_mask = torch.abs(param_matrix) > sorted_non_neg_param_1D.min()
	param_mask = param_mask.type(dtype)
	###if gpu is being used
	if torch.cuda.is_available():
		param_mask = param_mask.cuda()
	###
	return param_mask

"""### Train

"""

dtype = torch.FloatTensor

def trainCoxPASNet(train_x, train_age, train_ytime, train_yevent, \
			eval_x, eval_age, eval_ytime, eval_yevent, pathway_mask, \
			In_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, \
			Learning_Rate, L2, Num_Epochs, Dropout_Rate):

	net = Cox_PASNet(In_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, pathway_mask)
	###if gpu is being used
	if torch.cuda.is_available():
		net.cuda()
	###
	###optimizer
	losses_train = []
	losses_eval = []
	train_cindexes = []
	eval_cindexes = []

	opt = optim.Adam(net.parameters(), lr=Learning_Rate, weight_decay = L2)

	for epoch in range(Num_Epochs+1):
		net.train()
		opt.zero_grad() ###reset gradients to zeros
		###Randomize dropout masks
		net.do_m1 = dropout_mask(Pathway_Nodes, Dropout_Rate[0])
		net.do_m2 = dropout_mask(Hidden_Nodes, Dropout_Rate[1])

		# Forward pass for training data
		pred_train = net(train_x, train_age)
		loss_train = neg_par_log_likelihood(pred_train, train_ytime, train_yevent)
	  # Modification to handle potential multi-element loss
		if isinstance(loss_train, torch.Tensor) and len(loss_train.shape) > 0:
				loss_train = loss_train.mean() # Calculate the mean if loss_train is a multi-element tensor

		losses_train.append(loss_train.item())
		# Backward pass and optimization
		loss_train.backward() ###calculate gradients
		opt.step() ###update weights and biases

		net.sc1.weight.data = net.sc1.weight.data.mul(net.pathway_mask)

		# Calculate validation loss
		net.eval()
		with torch.no_grad():
			pred_eval = net(eval_x, eval_age)
			loss_eval = neg_par_log_likelihood(pred_eval, eval_ytime, eval_yevent)
			# Modification to handle potential multi-element loss
			if isinstance(loss_eval, torch.Tensor) and len(loss_eval.shape) > 0:
					loss_eval = loss_eval.mean()
			losses_eval.append(loss_eval.item())


		# Calculate C-index for training and validation data
		train_cindex = c_index(pred_train, train_ytime, train_yevent)
		eval_cindex = c_index(pred_eval, eval_ytime, eval_yevent)
		train_cindexes.append(train_cindex)
		eval_cindexes.append(eval_cindex)

		###obtain the small sub-network's connections
		do_m1_grad = copy.deepcopy(net.sc2.weight._grad.data)
		do_m2_grad = copy.deepcopy(net.sc3.weight._grad.data)
		do_m1_grad_mask = torch.where(do_m1_grad == 0, do_m1_grad, torch.ones_like(do_m1_grad))
		do_m2_grad_mask = torch.where(do_m2_grad == 0, do_m2_grad, torch.ones_like(do_m2_grad))
		###copy the weights
		net_sc2_weight = copy.deepcopy(net.sc2.weight.data)
		net_sc3_weight = copy.deepcopy(net.sc3.weight.data)

		###serializing net
		net_state_dict = net.state_dict()

		###Sparse Coding
		###make a copy for net, and then optimize sparsity level via copied net
		copy_net = copy.deepcopy(net)
		copy_state_dict = copy_net.state_dict()
		for name, param in copy_state_dict.items():
			###omit the param if it is not a weight matrix
			if not "weight" in name:
				continue
			###omit gene layer
			if "sc1" in name:
				continue
			###stop sparse coding
			if "sc4" in name:
				break
			###sparse coding between the current two consecutive layers is in the trained small sub-network
			if "sc2" in name:
				active_param = net_sc2_weight.mul(do_m1_grad_mask)
			if "sc3" in name:
				active_param = net_sc3_weight.mul(do_m2_grad_mask)
			nonzero_param_1d = active_param[active_param != 0]
			if nonzero_param_1d.size(0) == 0: ###stop sparse coding between the current two consecutive layers if there are no valid weights
				break
			copy_param_1d = copy.deepcopy(nonzero_param_1d)
			###set up potential sparsity level in [0, 100)
			S_set =  torch.arange(100, -1, -1)[1:]
			copy_param = copy.deepcopy(active_param)
			S_loss = []
			for S in S_set:
				param_mask = s_mask(sparse_level = S.item(), param_matrix = copy_param, nonzero_param_1D = copy_param_1d, dtype = dtype)
				transformed_param = copy_param.mul(param_mask)
				copy_state_dict[name].copy_(transformed_param)
				copy_net.train()
				y_tmp = copy_net(train_x, train_age)
				loss_tmp = neg_par_log_likelihood(y_tmp, train_ytime, train_yevent)

				S_loss.append(loss_tmp.mean().item() if isinstance(loss_tmp, torch.Tensor) else loss_tmp)

			###apply cubic interpolation
			# Convert S_loss elements to float if they are PyTorch scalars
			S_loss_values = [loss.item() if isinstance(loss, torch.Tensor) else loss for loss in S_loss]
			interp_S_loss = interp1d(S_set.detach().numpy(), S_loss_values, kind='cubic')
			interp_S_set = torch.linspace(min(S_set), max(S_set), steps=100)
			interp_loss = interp_S_loss(interp_S_set)
			optimal_S = interp_S_set[np.argmin(interp_loss)]
			optimal_param_mask = s_mask(sparse_level = optimal_S.item(), param_matrix = copy_param, nonzero_param_1D = copy_param_1d, dtype = dtype)
			if "sc2" in name:
				final_optimal_param_mask = torch.where(do_m1_grad_mask == 0, torch.ones_like(do_m1_grad_mask), optimal_param_mask)
				optimal_transformed_param = net_sc2_weight.mul(final_optimal_param_mask)
			if "sc3" in name:
				final_optimal_param_mask = torch.where(do_m2_grad_mask == 0, torch.ones_like(do_m2_grad_mask), optimal_param_mask)
				optimal_transformed_param = net_sc3_weight.mul(final_optimal_param_mask)
			###update weights in copied net
			copy_state_dict[name].copy_(optimal_transformed_param)
			###update weights in net
			net_state_dict[name].copy_(optimal_transformed_param)

		if epoch % 200 == 0:
			net.train()
			train_pred = net(train_x, train_age)
			# Calculate the mean of the loss if it's a multi-element tensor
			train_loss = neg_par_log_likelihood(train_pred, train_ytime, train_yevent)
			train_loss = train_loss.mean() if isinstance(train_loss, torch.Tensor) and len(train_loss.shape) > 0 else train_loss
			net.eval()
			eval_pred = net(eval_x, eval_age)
			# Calculate the mean of the loss if it's a multi-element tensor
			eval_loss = neg_par_log_likelihood(eval_pred, eval_ytime, eval_yevent)
			eval_loss = eval_loss.mean() if isinstance(eval_loss, torch.Tensor) and len(eval_loss.shape) > 0 else eval_loss

			train_cindex = c_index(train_pred, train_ytime, train_yevent)
			eval_cindex = c_index(eval_pred, eval_ytime, eval_yevent)
			print(f"Epoch {epoch} - Loss in Train: {train_loss.item()} - Loss in Validation: {eval_loss.item()}")
			print(f"Training C-index: {train_cindex}, Validation C-index: {eval_cindex}")

	# Plot the training and validation loss after training
	plt.figure(figsize=(10, 6))
	plt.plot(losses_train, label='Training Loss')
	plt.plot(losses_eval, label='Validation Loss')
	plt.xlabel('Epoch')
	plt.ylabel('Loss')
	plt.title('Training and Validation Loss Over Epochs')
	plt.legend()
	plt.grid(True)
	plt.show()

  # Plot the C-index values after training
	plt.figure(figsize=(10, 6))
	plt.plot(train_cindexes, label='Training C-index')
	plt.plot(eval_cindexes, label='Validation C-index')
	plt.xlabel('Epoch')
	plt.ylabel('C-index')
	plt.title('Training and Validation C-index Over Epochs')
	plt.legend()
	plt.grid(True)
	plt.show()



	return (train_loss, eval_loss, train_cindex, eval_cindex)

"""### Optuna Hyperparameter search spaces"""

# Define the Optuna objective function

def objective(trial):
    # Hyperparameter search spaces
    Learning_Rate = trial.suggest_loguniform('Learning_Rate',0.001, 0.1)
    L2 = trial.suggest_loguniform('L2', 0.001, 0.01)
    Num_Epochs = trial.suggest_int('Num_Epochs', 10, 300)
    Dropout_Rate = [trial.suggest_uniform('dropout_rate_1', 0.0, 0.7), trial.suggest_uniform('dropout_rate_2', 0.0, 0.5)]
    Hidden_Nodes = 300
    Pathway_Nodes = 300
    Out_Nodes = 2

    train_x, train_ytime, train_yevent, train_age, _, _ = load_data("/content/drive/MyDrive/jan_files/TRAINING.xlsx", dtype)
    eval_x, eval_ytime, eval_yevent, eval_age, _, _ = load_data("/content/drive/MyDrive/jan_files/VALIDATION.xlsx", dtype)
    pathway_mask = load_pathway("/content/drive/MyDrive/jan_files/pt_fixed.xlsx", dtype)

    In_Nodes = train_x.shape[1]  # Number of input nodes

# Train the model
    _, _, _, eval_cindex = trainCoxPASNet(train_x, train_age, train_ytime, train_yevent, \
        eval_x, eval_age, eval_ytime, eval_yevent, pathway_mask, \
        In_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, Learning_Rate, L2, Num_Epochs, Dropout_Rate)

    # Convert eval_cindex to a Python scalar if it's not already
    eval_cindex = eval_cindex.item() if isinstance(eval_cindex, torch.Tensor) else eval_cindex

    return eval_cindex  # Return the evaluation C-index

# Main execution block
if __name__ == "__main__":
    study = optuna.create_study(direction="maximize")  # Create a study to maximize C-index
    study.optimize(objective, n_trials=100)  # Optimize for 100 trials

    # Output best results
    print("Best hyperparameters: ", study.best_params)  # Print the best hyperparameters
    print("Best C-index: ", study.best_value)  # Print the best C-index value

"""### Train for interpret"""

dtype = torch.FloatTensor

def InterpretCoxPASNet(x, age, ytime, yevent, pathway_mask, \
						In_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, \
						Learning_Rate, L2, Num_Epochs, Dropout_Rate, outpath):

	net = Cox_PASNet(In_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, pathway_mask)
	###if gpu is being used
	if torch.cuda.is_available():
		net.cuda()
	###
	###optimizer
	opt = optim.Adam(net.parameters(), lr=Learning_Rate, weight_decay = L2)

	for epoch in range(Num_Epochs+1):
		net.train()
		opt.zero_grad() ###reset gradients to zeros
		###Randomize dropout masks
		net.do_m1 = dropout_mask(Pathway_Nodes, Dropout_Rate[0])
		net.do_m2 = dropout_mask(Hidden_Nodes, Dropout_Rate[1])

		pred = net(x, age) ###Forward
		loss = neg_par_log_likelihood(pred, ytime, yevent).mean()

		loss.backward()
		opt.step()

		net.sc1.weight.data = net.sc1.weight.data.mul(net.pathway_mask)

		###obtain the small sub-network's connections
		do_m1_grad = copy.deepcopy(net.sc2.weight._grad.data)
		do_m2_grad = copy.deepcopy(net.sc3.weight._grad.data)
		do_m1_grad_mask = torch.where(do_m1_grad == 0, do_m1_grad, torch.ones_like(do_m1_grad))
		do_m2_grad_mask = torch.where(do_m2_grad == 0, do_m2_grad, torch.ones_like(do_m2_grad))
		###copy the weights
		net_sc2_weight = copy.deepcopy(net.sc2.weight.data)
		net_sc3_weight = copy.deepcopy(net.sc3.weight.data)

		###serializing net
		net_state_dict = net.state_dict()

		###Sparse Coding
		###make a copy for net, and then optimize sparsity level via copied net
		copy_net = copy.deepcopy(net)
		copy_state_dict = copy_net.state_dict()
		for name, param in copy_state_dict.items():
			###omit the param if it is not a weight matrix
			if not "weight" in name:
				continue
			###omit gene layer
			if "sc1" in name:
				continue
			###stop sparse coding
			if "sc4" in name:
				break
			###sparse coding between the current two consecutive layers is in the trained small sub-network
			if "sc2" in name:
				active_param = net_sc2_weight.mul(do_m1_grad_mask)
			if "sc3" in name:
				active_param = net_sc3_weight.mul(do_m2_grad_mask)
			nonzero_param_1d = active_param[active_param != 0]
			if nonzero_param_1d.size(0) == 0: ###stop sparse coding between the current two consecutive layers if there are no valid weights
				break
			copy_param_1d = copy.deepcopy(nonzero_param_1d)
			###set up potential sparsity level in [0, 100)
			S_set =  torch.arange(100, -1, -1)[1:]
			copy_param = copy.deepcopy(active_param)
			S_loss = []
			for S in S_set:
					param_mask = s_mask(sparse_level = S.item(), param_matrix = copy_param, nonzero_param_1D = copy_param_1d, dtype = dtype)
					transformed_param = copy_param.mul(param_mask)
					copy_state_dict[name].copy_(transformed_param)
					copy_net.train()
					y_tmp = copy_net(x, age)
					loss_tmp = neg_par_log_likelihood(y_tmp, ytime, yevent)
					# Ensure loss_tmp is a scalar by taking the mean
					loss_tmp = loss_tmp.mean()
					S_loss.append(loss_tmp.item())
			###apply cubic interpolation
			interp_S_loss = interp1d(S_set.detach().numpy(), S_loss, kind='cubic')
			interp_S_set = torch.linspace(min(S_set), max(S_set), steps=100)
			interp_loss = interp_S_loss(interp_S_set)
			optimal_S = interp_S_set[np.argmin(interp_loss)]
			optimal_param_mask = s_mask(sparse_level = optimal_S.item(), param_matrix = copy_param, nonzero_param_1D = copy_param_1d, dtype = dtype)
			if "sc2" in name:
				final_optimal_param_mask = torch.where(do_m1_grad_mask == 0, torch.ones_like(do_m1_grad_mask), optimal_param_mask)
				optimal_transformed_param = net_sc2_weight.mul(final_optimal_param_mask)
			if "sc3" in name:
				final_optimal_param_mask = torch.where(do_m2_grad_mask == 0, torch.ones_like(do_m2_grad_mask), optimal_param_mask)
				optimal_transformed_param = net_sc3_weight.mul(final_optimal_param_mask)
			###update weights in copied net
			copy_state_dict[name].copy_(optimal_transformed_param)
			###update weights in net
			net_state_dict[name].copy_(optimal_transformed_param)

	###save the trained model
	torch.save(net.state_dict(), outpath)

	# Calculate and print the final loss on the full dataset
	net.eval()
	with torch.no_grad():
			final_pred = net(x, age)  # Forward pass with the full dataset
			final_loss = neg_par_log_likelihood(final_pred, ytime, yevent).mean()
			print(f"Final Loss: {final_loss.item()}")

	return

"""### Run for interpret = Actual Run"""

dtype = torch.FloatTensor

''' Net Settings'''
In_Nodes = 2722 ###number of genes
Pathway_Nodes = 300 ###number of pathways
Hidden_Nodes = 300 ###number of hidden nodes
Out_Nodes = 1 ###number of hidden nodes in the last hidden layer

''' Initialize with updated hyperparameters from empirical search '''
Initial_Learning_Rate = 0.01
L2_Lambda =  0.005
Num_EPOCHS = 1166
Dropout_Rate = [0.7, 0.5]

''' load data and pathway '''
pathway_mask = load_pathway("/content/drive/MyDrive/jan_files/pt_fixed.xlsx", dtype)
x, ytime, yevent, age, condition, feature_names = load_data("/content/drive/MyDrive/jan_files/entire_data.xlsx", dtype)

outpath = "/content/drive/MyDrive/jan_files/empirical_InterpretCoxPASNet.pt"

'''train Cox-PASNet for model interpretation'''
InterpretCoxPASNet(x, age, ytime, yevent, pathway_mask, \
					In_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, \
					Initial_Learning_Rate, L2_Lambda, Num_EPOCHS, Dropout_Rate, outpath)

'''load trained Cox-PASNet'''
net = Cox_PASNet(In_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, pathway_mask)
net.load_state_dict(torch.load(outpath))
###if gpu is being used
if torch.cuda.is_available():
	net.cuda()
###

'''save weights and node values into files individually'''
w_sc1 = net.sc1.weight.data.cpu().detach().numpy()
w_sc2 = net.sc2.weight.data.cpu().detach().numpy()
w_sc3 = net.sc3.weight.data.cpu().detach().numpy()
w_sc4 = net.sc4.weight.data.cpu().detach().numpy()
np.savetxt("/content/drive/MyDrive/jan_files/empirical_w_sc1.csv", w_sc1, delimiter = ",")
np.savetxt("/content/drive/MyDrive/jan_files/empirical_w_sc2.csv", w_sc2, delimiter = ",")
np.savetxt("/content/drive/MyDrive/jan_files/empirical_w_sc3.csv", w_sc3, delimiter = ",")
np.savetxt("/content/drive/MyDrive/jan_files/empirical_w_sc4.csv", w_sc4, delimiter = ",")
# Forward pass for intermediate nodes
pathway_node = net.tanh(net.sc1(x))
hidden_node = net.tanh(net.sc2(pathway_node))
hidden_2_node = net.tanh(net.sc3(hidden_node))
x_cat = torch.cat((hidden_2_node, age), 1)
lin_pred = net.sc4(x_cat)

np.savetxt("/content/drive/MyDrive/jan_files/empirical_pathway_node.csv", pathway_node.cpu().detach().numpy(), delimiter = ",")
np.savetxt("/content/drive/MyDrive/jan_files/empirical_hidden_node.csv", hidden_node.cpu().detach().numpy(), delimiter = ",")
np.savetxt("/content/drive/MyDrive/jan_files/empirical_hidden_2_node.csv", x_cat.cpu().detach().numpy(), delimiter = ",")
np.savetxt("/content/drive/MyDrive/jan_files/empirical_lin_pred.csv", lin_pred.cpu().detach().numpy(), delimiter = ",")

"""### Save the model"""

# Save the model
with open('model_optuna.pkl', 'wb') as file:
    pickle.dump(net, file)

"""### Load the model as net"""

# Load the model
with open('model_optuna.pkl', 'rb') as file:
    net = pickle.load(file)

"""### Shap analysis"""

''' SHAP Analysis for Interpretability '''

# Ensure dimensions match for concatenation
x_combined = torch.cat((x, condition), dim=1)

# Define a wrapper for the model
class NetWrapper(torch.nn.Module):
    def __init__(self, net):
        super(NetWrapper, self).__init__()
        self.net = net

    def forward(self, x):
        return self.net(x[:, :-1], x[:, -1])

# Wrap the trained model for SHAP
wrapped_net = NetWrapper(net)

# Perform SHAP analysis
explainer = shap.DeepExplainer(wrapped_net, x_combined)
shap_values = explainer.shap_values(x_combined)

# Update feature names to include Condition
feature_names += ['Condition']

# Generate SHAP summary plots
shap.summary_plot(shap_values, x_combined, plot_type="bar", feature_names=feature_names, max_display=50, show=False)
plt.savefig("/content/drive/MyDrive/jan_files/empirical_shap_summary_bar_50.svg")

shap.summary_plot(shap_values, x_combined, plot_type="bar", feature_names=feature_names, max_display=20, show=False)
plt.savefig("/content/drive/MyDrive/jan_files/empirical_shap_summary_bar_20.svg")